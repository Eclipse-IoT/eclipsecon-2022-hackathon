
:experimental:
:icons: font

== Pre-requisites

* Installed ACM/OCM instance
  ** Install operator
  ** Create `MultiClusterHub` resource

== OCM/ACM

NOTE: OCM/ACM needs to be installed and set up once. For adding a new gateway see <<Add new gateway>>.

Also see:

  * https://open-cluster-management.io/[Open Cluster Management (OCM)]
  * https://www.redhat.com/en/technologies/management/advanced-cluster-management[Red Hat Advanced Cluster Management for Kubernetes (ACM)]

=== Pull secret

When creating a `MultiClusterHub` resource, you will need to configure a "pull secret", if using ACM.

Create the secret using:

[source,shell]
----
oc apply -n open-cluster-management -f pull-secret.yaml
----

=== Create the cluster hub

Note the secret name, and apply it to the `MultiClusterHub` resource:

[source,yaml]
----
apiVersion: operator.open-cluster-management.io/v1
kind: MultiClusterHub
metadata:
  name: multiclusterhub
  namespace: open-cluster-management
spec:
  imagePullSecret: <secret>
----

=== (Optional) Create a cluster set

Create a "ClusterSet" (`ManagedClusterSet`):

[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: ManagedClusterSet
metadata:
  name: eclipsecon
spec:
  clusterSelector:
    selectorType: LegacyClusterSetLabel
----

And a `ManagedClusterSetBinding`:

[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: ManagedClusterSetBinding
metadata:
  name: eclipsecon
  namespace: eclipsecon-2022
spec:
  clusterSet: eclipsecon
----

== Add new gateway

* Install dependencies
* Install https://microshift.io[MicroShift]
* Connect with ACM

=== Install dependencies

Wifi firmware:

[source,shell]
----
sudo dnf install iwl7260-firmware NetworkManager-wifi
sudo reboot
----

Then:

[source,shell]
----
sudo nmcli device wifi connect <SSID> password <password>
----

Bluez:

[source,shell]
----
sudo dnf install bluez
sudo systemctl enable bluetooth --now
----

=== Install MicroShift

Install a most recent Fedora on the target machine.

Then run the following commands to install pre-requisites:

[source,shell]
----
sudo dnf module enable -y cri-o:1.21
sudo dnf install -y cri-o cri-tools
sudo systemctl enable crio --now
----

Next, install MicroShift itself:

[source,shell]
----
sudo dnf copr enable -y @redhat-et/microshift
sudo dnf install -y microshift
sudo firewall-cmd --zone=trusted --add-source=10.42.0.0/16 --permanent
sudo firewall-cmd --zone=public --add-port=80/tcp --permanent
sudo firewall-cmd --zone=public --add-port=443/tcp --permanent
sudo firewall-cmd --zone=public --add-port=5353/udp --permanent
sudo firewall-cmd --reload
sudo systemctl enable microshift --now
----

==== Troubleshoot

In case you have issues with MicroShift installation:

* Remove the installation

[source,shell]
----
sudo systemctl remove microshift
sudo dnf remove -y microshift
----

* Force Fedora 35 build by editing
`/etc/yum.repos.d/_copr\:copr.fedorainfracloud.org\:group_redhat-et\:microshift.repo` file
and replace `$releasever` with `35` in the `baseurl`, like

[source]
----
baseurl=https://download.copr.fedorainfracloud.org/results/@redhat-et/microshift/fedora-35-$basearch/
----

* Install MicroShift again

[source,shell]
----
sudo dnf install -y microshift
sudo systemctl enable microshift --now
----

=== Command line access on the gateway

Install the `oc` binary:

[source,shell]
----
curl -O https://mirror.openshift.com/pub/openshift-v4/$(uname -m)/clients/ocp/stable/openshift-client-linux.tar.gz
sudo tar -xf openshift-client-linux.tar.gz -C /usr/local/bin oc kubectl
----

And create the "kubeconfig" file (as an ordinary user):

[source,shell]
----
mkdir ~/.kube
sudo cat /var/lib/microshift/resources/kubeadmin/kubeconfig > ~/.kube/config
----

=== Check

You can see if pods are up using:

[source,shell]
----
oc get pods -A
----

=== Add to ACM

Console: https://multicloud-console.apps.sandbox.drogue.world

Import a new cluster:

* Choose a name
* Select the cluster set: `eclipsecon`
* Import mode: "Run import commands manually"

When the cluster is created, use the btn:[Copy command] button to load the script into the clipboard. Then paste the
content (~45KiB) into a bash, or create a new file with its content (e.g. `install.sh`), and run it (e.g. using `bash install.sh`).

The output should look like:

[source]
----
customresourcedefinition.apiextensions.k8s.io/klusterlets.operator.open-cluster-management.io created
namespace/open-cluster-management-agent created
serviceaccount/klusterlet created
clusterrole.rbac.authorization.k8s.io/klusterlet created
clusterrole.rbac.authorization.k8s.io/open-cluster-management:klusterlet-admin-aggregate-clusterrole created
clusterrolebinding.rbac.authorization.k8s.io/klusterlet created
deployment.apps/klusterlet created
secret/bootstrap-hub-kubeconfig created
klusterlet.operator.open-cluster-management.io/klusterlet created
----

== Remove a node

* Detach the node from ACM
* Uninstall MicroShift

=== Uninstall MicroShift

Disable the service:

[source,shell]
----
sudo systemctl disable microshift --now
----

Uninstall the RPM:

[source,shell]
----
sudo dnf remove microshift
----

=== Detaching manually

It might happen that the cluster cannot be detached. Re-attaching it will then fail with the following message:

[source]
----
Error from server (AlreadyExists): error when creating "STDIN": customresourcedefinitions.apiextensions.k8s.io "klusterlets.operator.open-cluster-management.io" already exists
The cluster cannot be imported because its Klusterlet CRD already exists.
Either the cluster was already imported, or it was not detached completely during a previous detach process.
Detach the existing cluster before trying the import again.‚èé
----

The following steps will clean this situation up:

[source,shell]
----
oc delete klusterlets.operator.open-cluster-management.io klusterlet
oc delete ns open-cluster-management-agent
oc delete ns open-cluster-management-agent-addon
oc delete crd klusterlets.operator.open-cluster-management.io
----

=== Rollout the gateway application

You can deploy gateway on each cluster in a set by creating a `placement` and appropriate work for it

[source,shell]
----
kubectl apply -n eclipsecon-2022 -f placement.yaml
clusteradm create work eclipsecon-gateway -f gateway-daemonset.yaml --placement gateway
----

=== Rollout the gateway application using work manifest

For each gateway, create the following resource in its (gateway) namespace:

[source,yaml]
----
include::work.yaml[]
----

=== Remove gateway application

To remove the application from the cluster run:

[source,shell]
----
clusteradm delete work eclipsecon-gateway --cluster <CLUSTER_NAME>
----
